# TicTacToe-RL
<br>
<h2>Задачи:</h2> 

- Разработать RL-агента;
- В качестве основы алгоритма RL использовать уравнение Беллмана;
- Агент ходит не однообразно;
- Добиться качественных статистических показателей игры агента:
  
  a) у агента невозможно выйграть;
  
  б) агент всегда ходит в пользу своей победы, сводя к максимально возможному минимуму процент ничьих.

<br>
<h2>Решение 1:</h2> 

<h4>Принцип RL - уравнение Беллмана:</h4>

![i2_ugxlinshqmsyzkawlbmirxri-fotor-2023113004810](https://github.com/WhiteSpite/TicTacToe-RL/assets/113059464/ffe4ad1a-c0fd-45fc-9d27-ceabe9da15f7)

<h4>Основная проблема:</h4> 

Для какого следующего состояния находить max Q (в целях обновления Q текущего хода)?:
1) Следующее состояние игрового стола, которое мы имеем после хода нашего агента брать некорректно, т.к. из него агент уже не сможет походить, т.к. ход переходит к противнику;
2) После хода нашего агента можно предсказывать ход противника на основе q таблицы нашего агента, и уже для состояния игрового стола, которое получится после теоритического хода противника, находить max Q;
3) Также можно попробовать находить Q max для состояния к которому приводит наш ход, предварительно инвертировав это состояние (X заменить на O, а О заменить на X) стола и взяв Q max со знаком -. Логика в том, что чем больше предполагаемое значение Q max для состояния стола которое получает наш противник после нашего хода, тем менее ценным это состояние должно быть для нас.

<h4>Результаты:</h4> 

- Были испробованы все 3 метода, при каждом агент улучшал статистические показатели в процессе обучения, особенно при использовании 1-ого метода (вероятно потому что во 2-ом и 3-ем случае корректность определения Q max сильно зависит от хода противника, однако в 1-ом случае Q распространяется некорректно в результате логической ошибки упомянутой выше);
- Однако результаты были не идеальны и сильно зависели от уровня рандомности агента-противника;
- Игра с изменением конфигурации (gamma, alpha, epsilon, награды за победу, поражение, ничью и ход) не привела к желаемым результатам;
- Принято решение попробовать иные алгоритмы RL обучения.

<br>
<h2>Решение 2:</h2> 

<h4>Принцип RL:</h4> 

- Агент запоминает все сделанные им ходы;
- После окончания игры агент обновляет ценность этих ходов соответственно наградам за победу, поражение или ничью.

<h4>Результаты:</h4>

- При alpha < 1, а также при динамически снижаемом alpha, агент не показал идеальные статистические показатели в игре с рандомом;
- При alpha == 1 агент научился показывать идеальные статистические результаты с противником любого уровня рандомности;
- Единственный недостаток - агент ходит в большинстве случаев однообразно, и изменить у меня это не получилось;
- Принято решение попробовать иные алгоритмы RL обучения.

<h4>График обучения (alpha == 1) при игре с противником который ходит рандомно:</h4>

![RL_vs_random](https://github.com/WhiteSpite/TicTacToe-RL/assets/113059464/bcbc8b11-a7b7-4d4e-91b5-c15d31e07fbe)

Из графика можно видеть, что для полного обучения агенту достаточно провести ~8823 игры.
Достигнутая статистика:
- Победы: 94%;
- Поражения: 0%;
- Ничьи: 5%;

<br>
<h2>Решение 3:</h2>

<h4>Принцип RL:</h4> 

- Пока обучение не завершено, наш агент ходит исключительно с epsilon равным 1;
- Пока обучение не завершено, наш агент играет исключительно с развитым противником с epsilon равным ~0.7;
- К каждому из возможных ходов по всей Q таблице агента пристваиваются три счетчика для побед, поражений и ничьих соответственно;
- Агент запоминает все сделанные им ходы;
- После окончания игры агент увеличивает соответствующий результату игры счетчик на 1 для каждого из запомненных ходов;
- Агент учится на максимально возможном количестве игр;
- После окончания обучения в качестве Q каждого из ходов в Q таблице устанавливается среднее арифмитическое между счетчиками соответствующего хода;
- Затем выполняется кластеризация (метод k-средних) Q всех ходов в Q таблице на минимально возможное (не влияющее на статистику агента) количество кластеров (в целях минимизации однообразности игры);
- В качестве новых Q для каждого из ходов в Q таблице устанавливаются лейблы соответствующих кластеров.

<h4>Результаты:</h4> 




