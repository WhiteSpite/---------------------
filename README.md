# TicTacToe-RL

<h2>Задачи:</h2> 

- Разработать RL-агента;
- В качестве основы алгоритма RL использовать уравнение Беллмана;
- Агент ходит не однообразно;
- Добиться качественных статистических показателей игры агента:
  
  a) у агента невозможно выйграть;
  
  б) агент всегда ходит в пользу своей победы, сводя к максимально возможному минимуму процент ничьих.


<h2>Решение 1:</h2> 

<h4>Принцип RL - уравнение Беллмана:</h4>

![i2_ugxlinshqmsyzkawlbmirxri-fotor-2023113004810](https://github.com/WhiteSpite/TicTacToe-RL/assets/113059464/ffe4ad1a-c0fd-45fc-9d27-ceabe9da15f7)

<h4>Основная проблема:</h4> 

Для какого следующего состояния находить max Q (в целях обновления Q текущего действия)?:
1) Следующее состояние игрового стола, которое мы имеем после хода нашего агента брать некорректно, т.к. из этого агент уже не сможет походить, т.к. ход переходит к противнику;
2) После хода нашего агента можно предсказывать ход противника на основе q таблицы нашего агента, и уже для состояния игрового стола, которое получится после теоритического хода противника, находить max Q;
3) Также можно попробовать находить Q max для состояния к которому приводит наш ход, предварительно инвертировав это состояние стола и взяв Q max со знаком -. Логика в том, что чем больше предполагаемое значение Q max для состояния стола которое получает наш противник после нашего хода, тем менее ценным это состояние должно быть для нас.

<h4>Результаты:</h4> 

- Были испробованы все 3 метода, при каждом агент улучшал статистические показатели в процессе обучения, особенно при использовании 1-ого метода (вероятно потому что во 2-ом и 3-ем случае корректность определения Q max сильно зависит от хода противника);
- Однако результаты были не идеальны и сильно зависели от уровня рандомности агента-противника.
- Игра с конфигурацией (гамма, альфа, эпсилон, награды за победу, поражение, ничью и ход) не привела к желаемым результатам.
- Принято решение попробовать иные алгоритмы RL обучения.


<h2>Решение 2:</h2> 

<h4>Принцип RL:</h4> 

- Агент запоминает все<br> сделанные им ходы;
- После окончания игры агент обновляет ценность этих действий соответственно наградам за победу, поражение или ничью.

<h4>Результаты:</h4>

- При alpha < 1, а также при динамически снижаемом alpha, агент не показал идеальные статистические показатели в игре с рандомом
- При alpha == 1 агент научился показывать идеальные статистические результаты с противником любого уровня рандомности.
- Единственный недостаток - агент ходит в большинстве случаев однообразно, и изменить у меня это не получилось.
- Принято решение попробовать иные алгоритмы RL обучения.

<h4>График обучения (alpha == 1) при игре с противником который ходит рандомно:</h4>

![RL_vs_random](https://github.com/WhiteSpite/TicTacToe-RL/assets/113059464/bcbc8b11-a7b7-4d4e-91b5-c15d31e07fbe)

Из графика можно видеть, что для полного обучения агенту достаточно провести ~8823 игры.
Достигнутая статистика:
- Победы: 94%;
- Поражения: 0%;
- Ничьи: 5%;


<h2>Решение 3:</h2>
